# -*- coding: utf-8 -*-
"""chatBot

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RekWpQrX5ef_maDj7HLD0eoCuc18goFr
"""

!nvidia-smi

pip install transformers

pip install torch

import numpy as np
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
import json
from torch.utils.data import Dataset
from torch.optim import Adam
from torch.utils.data import DataLoader
import tqdm
import torch

tokenizer= GPT2Tokenizer.from_pretrained("gpt2")
tokenizer.add_special_tokens({"pad_token": "<pad>",
                                "bos_token": "<startofstring>",
                                "eos_token": "<endofstring>"})
tokenizer.add_tokens(["<bot>:"])
model= GPT2LMHeadModel.from_pretrained("gpt2")
model.resize_token_embeddings(len(tokenizer))
model.resize_token_embeddings(len(tokenizer))
device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
model = model.to(device)

def train(chatData, model, optim):

    epochs = 12

    for i in tqdm.tqdm(range(epochs)):
        for X, a in chatData:
            X = X.to(device)
            a = a.to(device)
            optim.zero_grad()
            loss = model(X, attention_mask=a, labels=X).loss
            loss.backward()
            optim.step()
        torch.save(model.state_dict(), "model_state.pt")
        print(infer("hello how are you"))

def infer(inp):
    inp = "<startofstring> "+inp+" <bot>: "
    inp = tokenizer(inp, return_tensors="pt")
    X = inp["input_ids"].to(device)
    a = inp["attention_mask"].to(device)
    output = model.generate(X, attention_mask=a )
    output = tokenizer.decode(output[0])
    return output

class Datasets(Dataset):
    def __init__(self, path:str,tokenizer):
        self.data=json.load(open(path,'r'))

        self.X=[]
        for i in self.data:
            for j in i['dialog']:
                self.X.append(j['text'])

        for idx, i in enumerate(self.X):
            try:
                self.X[idx] = "<startofstring> "+i+" <bot>: "+self.X[idx+1]+" <endofstring>"
            except:
                break

        self.X = self.X[:5000]

        print(self.X[0])

        self.X_encoded = tokenizer(self.X,max_length=40, truncation=True, padding="max_length", return_tensors="pt")
        self.input_ids = self.X_encoded['input_ids']
        self.attention_mask = self.X_encoded['attention_mask']

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        return (self.input_ids[idx], self.attention_mask[idx])

path="chat_data.json"
chatData=Datasets(path,tokenizer)
chatData=DataLoader(chatData,batch_size=64)

model.train()
optim = Adam(model.parameters(), lr=1e-3)

print("training .... ")
train(chatData, model, optim)

print("infer from model : ")

while True:
  inp = input()
  print(infer(inp))